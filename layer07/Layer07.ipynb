{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0PWRgNdKdIs"
   },
   "source": [
    "# **Layer 12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwJuAeaLKwn6"
   },
   "source": [
    "## **Prepare Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Sswa4lVLt61"
   },
   "source": [
    "### Import libraries and modules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T13:45:32.614056600Z",
     "start_time": "2023-09-22T13:45:32.567137800Z"
    },
    "id": "Z6RZkkxdLmww"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umWGnfGFMQUq"
   },
   "source": [
    "### Load the dataset\n",
    "*   Training Data\n",
    "*   Validation Data\n",
    "*   Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T13:45:29.660769300Z",
     "start_time": "2023-09-22T13:45:29.629654600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAAmKpHzMKv6",
    "outputId": "000eca06-6e20-4992-a218-a28e4abad964"
   },
   "outputs": [],
   "source": [
    "train_path = 'D:\\\\ACADEMIC\\\\SEMESTER 07\\\\ML - 3\\\\project\\\\Layer07\\\\train.csv'\n",
    "valid_path = 'D:\\\\ACADEMIC\\\\SEMESTER 07\\\\ML - 3\\\\project\\\\Layer07\\\\valid.csv'\n",
    "test_path = 'D:\\\\ACADEMIC\\\\SEMESTER 07\\\\ML - 3\\\\project\\\\Layer07\\\\test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "\n",
    "valid_data = pd.read_csv(valid_path)\n",
    "\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B1ENntLQ9Gg"
   },
   "source": [
    "### Dataset Info and split dataset according to the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r47wVJffQ4BM",
    "outputId": "4a95a3b6-1cec-46a9-be9b-ca718db3f58c"
   },
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "ZMVSYcSOSfM0",
    "outputId": "3a3013c5-d37b-4214-e37a-71b5503ae15b"
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "F8ONSTT5SkET",
    "outputId": "de02c9ea-18b3-4d32-9fe4-e49437929d4b"
   },
   "outputs": [],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "DbrFHC0dWwE1",
    "outputId": "c8645b4e-35ff-4937-89da-b75a93cfcbdb"
   },
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgfJPL3YW7Gv"
   },
   "source": [
    "Drop the ID column from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0z6hL0jWycN"
   },
   "outputs": [],
   "source": [
    "test_IDs = test_data['ID'].to_numpy()\n",
    "test_data = test_data.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7La420gAYKtb"
   },
   "source": [
    "Prepare training and validation data for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLiwYHqRX6uO"
   },
   "outputs": [],
   "source": [
    "train_data_label1 = train_data.drop(columns=['label_2', 'label_3', 'label_4'])\n",
    "train_data_label2 = train_data.drop(columns=['label_1', 'label_3', 'label_4'])\n",
    "train_data_label3 = train_data.drop(columns=['label_1', 'label_2', 'label_4'])\n",
    "train_data_label4 = train_data.drop(columns=['label_1', 'label_2', 'label_3'])\n",
    "\n",
    "valid_data_label1 = valid_data.drop(columns=['label_2', 'label_3', 'label_4'])\n",
    "valid_data_label2 = valid_data.drop(columns=['label_1', 'label_3', 'label_4'])\n",
    "valid_data_label3 = valid_data.drop(columns=['label_1', 'label_2', 'label_4'])\n",
    "valid_data_label4 = valid_data.drop(columns=['label_1', 'label_2', 'label_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89ITqDiq-upk"
   },
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5EVQB5zAuo8"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAO1u_SM_RvG"
   },
   "source": [
    "> *Train a model to predict the label 01 after appling some feature engineering techniques and methods to the training data.\n",
    "Features are selected based on the correlation matrix and the PCA used to extract the features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0_4PjoBAocZ"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGVPy9vKBBMR"
   },
   "source": [
    "\n",
    "\n",
    "> Remove null values for labels and determine missing values in features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GauDtDG8F6i7"
   },
   "source": [
    "**Drop** the rows where there are null values for the lables in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTPGeSE6YZ_e",
    "outputId": "57b489ce-11a0-41c7-ac89-9f7336795878"
   },
   "outputs": [],
   "source": [
    "def clean_null_labels(train_data_label1, label):\n",
    "    print(\"Train set shape before: {}\".format(train_data_label1.shape))\n",
    "\n",
    "    train_features_null_counts = train_data_label1.drop(columns=[f'label_{label}']).isnull().sum()\n",
    "    train_label_null_count = train_data_label1[f'label_{label}'].isnull().sum()\n",
    "    print(\"Null value counts of the features\\n{}\".format(train_features_null_counts))\n",
    "    print(\"Null value count: {}\".format(train_label_null_count))\n",
    "\n",
    "    cleaned = train_data_label1.dropna(subset=train_data_label1.columns[-1:], how='any')\n",
    "    print(\"Train set shape after: {}\".format(cleaned.shape))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu4qF7_jFsna"
   },
   "source": [
    "Fill the null values in the features with their **means** in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XERatcIdCDUD"
   },
   "outputs": [],
   "source": [
    "def fill_null_features(train_data_label1, valid_data_label1, test_data):\n",
    "    train_data_label1 = train_data_label1.fillna(train_data_label1.mean())\n",
    "    valid_data_label1 = valid_data_label1.fillna(valid_data_label1.mean())\n",
    "    test_data = test_data.fillna(test_data.mean())\n",
    "    return train_data_label1, valid_data_label1, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXncTN4iGSCq"
   },
   "source": [
    "Split the Features and Labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgrscOI8GK7N"
   },
   "outputs": [],
   "source": [
    "def split_features_labels(train_data_label1, valid_data_label1, test_data, label):\n",
    "    train_features_label1 = train_data_label1.iloc[:, :-1]\n",
    "    train_label1 = train_data_label1[f'label_{label}']\n",
    "\n",
    "    valid_features_label1 = valid_data_label1.iloc[:, :-1]\n",
    "    valid_label1 = valid_data_label1[f'label_{label}']\n",
    "\n",
    "    test_features_label1 = test_data\n",
    "    return train_features_label1, train_label1, valid_features_label1, valid_label1, test_features_label1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aM68t1rHePY"
   },
   "source": [
    "Label 01 distribution after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "0m7qU-KYGt4G",
    "outputId": "6bcb05af-bb47-4ae4-8816-62b2bf6e795a"
   },
   "outputs": [],
   "source": [
    "def plot_label(train_label1, label):    \n",
    "    labels, counts = np.unique(train_label1, return_counts=True)\n",
    "\n",
    "    plt.figure(figsize=(18, 3))\n",
    "    plt.xticks(labels)\n",
    "    plt.bar(labels, counts)\n",
    "    plt.xlabel(f'label_{label}')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of the Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzlcHhaPIHQx"
   },
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-mu8uhsIS_q"
   },
   "source": [
    "> Scale the features of the dataset using **Robust scaler/ StandardScaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23WWgkXYHcvy"
   },
   "outputs": [],
   "source": [
    "def standardize_data(train_features_label1, valid_features_label1, test_features_label1, scaler=RobustScaler()): \n",
    "    standardized_train_features_label1 = scaler.fit_transform(train_features_label1)\n",
    "    standardized_valid_features_label1 = scaler.transform(valid_features_label1)\n",
    "    standardized_test_features_label1 = scaler.transform(test_features_label1)\n",
    "    return standardized_train_features_label1, standardized_valid_features_label1, standardized_test_features_label1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb8ymUTbIo_m"
   },
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53vt06AyIvnr"
   },
   "source": [
    "> Principal Componenet Analysis(PCA) used to extract the features that can explain the variance of the label to 95% and display the resulting explained variances of each PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "UKZT8yQZIhEr",
    "outputId": "2391cce2-b13d-4ab7-db9b-369eec1d7fdb"
   },
   "outputs": [],
   "source": [
    "def apply_PCA(standardized_train_features_label1, standardized_valid_features_label1, standardized_test_features_label1, variance_threshold):\n",
    "\n",
    "    pca = PCA(n_components=variance_threshold, svd_solver='full')\n",
    "\n",
    "    pca_train_features_label1 = pca.fit_transform(standardized_train_features_label1)\n",
    "    pca_valid_features_label1 = pca.transform(standardized_valid_features_label1)\n",
    "    pca_test_features_label1 = pca.transform(standardized_test_features_label1)\n",
    "\n",
    "    explained_variance_ratio_reduced = pca.explained_variance_ratio_\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.bar(range(1, pca_train_features_label1.shape[1] + 1), explained_variance_ratio_reduced)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance Ratio per Principal Component (Reduced)')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nReduced Train feature matrix shape: {}\".format(pca_train_features_label1.shape))\n",
    "    print(\"Reduced valid feature matrix shape: {}\".format(pca_valid_features_label1.shape))\n",
    "    print(\"Reduced test feature matrix shape: {}\".format(pca_test_features_label1.shape))\n",
    "    \n",
    "    return pca_train_features_label1, pca_valid_features_label1, pca_test_features_label1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iG5x5ozpHIzs"
   },
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsOhUhRFHmHe"
   },
   "source": [
    "Define parameters for random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QzNxRu2Hjxg"
   },
   "outputs": [],
   "source": [
    "def get_hyper_params():\n",
    "    svm_grid_params = {\n",
    "        'C': [0.1, 1, 10, 100, 1000],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "        'kernel': ['rbf','linear']\n",
    "    }\n",
    "\n",
    "    knn_grid_params = {\n",
    "        'n_neighbors' : [3, 5, 7, 9, 11, 13],\n",
    "        'weights' : ['uniform', 'distance'],\n",
    "        'metric' : ['minkowski', 'euclidean', 'manhattan', 'hamming']\n",
    "    }\n",
    "\n",
    "    random_forest_grid_params = {\n",
    "        'bootstrap': [True, False],\n",
    "        'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "    }\n",
    "    return svm_grid_params, knn_grid_params, random_forest_grid_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGKKVOGDa2x6"
   },
   "source": [
    "Tune hyperparameters with the best method by testing severel methods and for several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6heHCySOa_rz",
    "outputId": "ef84867f-08c2-4612-a030-62b0beeffe1f"
   },
   "outputs": [],
   "source": [
    "def tune_hyper_params(svm_grid_params, knn_grid_params, rf_grid_params, pca_train_features_label1, train_label1, rand=True):\n",
    "    classification_models_params = [\n",
    "        ('SVM', SVC(), svm_grid_params),\n",
    "    #     ('K Neighbors', KNeighborsClassifier(), knn_grid_params),\n",
    "    #     ('Random Forest', RandomForestClassifier(), rf_grid_params)\n",
    "    ]\n",
    "\n",
    "    for model_name, model, grid_params in classification_models_params:\n",
    "        if rand:\n",
    "            search = RandomizedSearchCV(\n",
    "                estimator = model,\n",
    "                param_distributions = grid_params,\n",
    "                n_iter = 40, cv = 3, verbose=4, random_state=42, n_jobs = -1\n",
    "            )\n",
    "        else:\n",
    "            search = HalvingGridSearchCV(\n",
    "                estimator=model,\n",
    "                param_grid=grid_params,\n",
    "                cv=3,\n",
    "                n_jobs=-1,\n",
    "                factor=2,\n",
    "                verbose=2\n",
    "            )\n",
    "        result = search.fit(pca_train_features_label1, train_label1)\n",
    "\n",
    "        print(f\"best score for {model_name} : {result.best_score_}\")\n",
    "        print(f\"best hyper parameters for {model_name} : {result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the model that best predicts the valid and test datasets based on accuracy, precision and recall and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HwLQtcazyUa",
    "outputId": "33d08be7-b9eb-4d19-e0fa-4e22f9fb5dc4"
   },
   "outputs": [],
   "source": [
    "def train_model(pca_train_features_label1, train_label1, pca_valid_features_label1, valid_label1, svm=None, rf=None, knn=None):\n",
    "    classification_models = [\n",
    "        # ('K Neighbors', knn),\n",
    "        # ('Random Forest', rf),\n",
    "        ('SVM', svm)\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    models = []\n",
    "\n",
    "    for model_name, model in classification_models:\n",
    "        num_features = pca_train_features_label1.shape[1]\n",
    "        print(f\"{model_name} is training for {num_features} number of features\\n\")\n",
    "        \n",
    "        models.append(model)\n",
    "        kf = KFold(n_splits=3, random_state=42, shuffle=True)\n",
    "        cross_val_scores = cross_val_score(model, pca_train_features_label1, train_label1, cv=kf, verbose=4)\n",
    "\n",
    "        print(\"CV Accuracy: %0.4f accuracy with a standard deviation of %0.2f\" % (cross_val_scores.mean(), cross_val_scores.std()))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_result(pca_train_features_label1, train_label1, pca_test_features_label1, model): \n",
    "    model.fit(pca_train_features_label1, train_label1)\n",
    "    return model.predict(pca_test_features_label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(ID, pred_label1, destination):\n",
    "  df = pd.DataFrame()\n",
    "\n",
    "  df.insert(loc=0, column='ID', value=ID)\n",
    "  df.insert(loc=1, column='label_1', value=pred_label1)\n",
    "\n",
    "  df.to_csv(destination, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_label1 = clean_null_labels(train_data_label1, 1)\n",
    "train_data_label1, valid_data_label1, test_data = fill_null_features(train_data_label1, valid_data_label1, test_data)\n",
    "train_features_label1, train_label1, valid_features_label1, valid_label1, test_features_label1 = split_features_labels(train_data_label1, valid_data_label1, test_data, 1)\n",
    "plot_label(train_label1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_train_features_label1, standardized_valid_features_label1, standardized_test_features_label1 = standardize_data(train_features_label1, valid_features_label1, test_features_label1, StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train_features_label1, pca_valid_features_label1, pca_test_features_label1 = apply_PCA(standardized_train_features_label1, standardized_valid_features_label1, standardized_test_features_label1, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_grid_params, knn_grid_params, random_forest_grid_params = get_hyper_params()\n",
    "# tune_hyper_params(svm_grid_params, knn_grid_params, random_forest_grid_params, pca_train_features_label1, train_label1, rand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = train_model(standardized_train_features_label1, train_label1, standardized_valid_features_label1, valid_label1, SVC(kernel=\"rbf\", C=100, gamma=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_label1 = get_test_result(standardized_train_features_label1, train_label1, standardized_test_features_label1, model1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = 'D:\\\\ACADEMIC\\\\SEMESTER 07\\\\ML - 3\\\\project\\\\Layer07\\\\results\\\\01.csv'\n",
    "\n",
    "create_csv(test_IDs, y_pred_test_label1, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_label2 = clean_null_labels(train_data_label2, 2)\n",
    "train_data_label2, valid_data_label2, test_data = fill_null_features(train_data_label2, valid_data_label2, test_data)\n",
    "train_features_label2, train_label2, valid_features_label2, valid_label2, test_features_label2 = split_features_labels(train_data_label2, valid_data_label2, test_data, 2)\n",
    "plot_label(train_label2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_train_features_label2, standardized_valid_features_label2, standardized_test_features_label2 = standardize_data(train_features_label2, valid_features_label2, test_features_label2, StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train_features_label2, pca_valid_features_label2, pca_test_features_label2 = apply_PCA(standardized_train_features_label2, standardized_valid_features_label2, standardized_test_features_label2, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_grid_params, knn_grid_params, random_forest_grid_params = get_hyper_params()\n",
    "# tune_hyper_params(svm_grid_params, knn_grid_params, random_forest_grid_params, pca_train_features_label2, train_label2, rand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = train_model(standardized_train_features_label2, train_label2, standardized_valid_features_label2, valid_label2, SVC(kernel='rbf', C=100, gamma=0.001, class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_label2 = get_test_result(standardized_train_features_label2, train_label2, standardized_test_features_label2, model2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = 'D:\\\\ACADEMIC\\\\SEMESTER 07\\\\ML - 3\\\\project\\\\Layer07\\\\results\\\\02.csv'\n",
    "\n",
    "create_csv(test_IDs, y_pred_test_label2, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_label3 = clean_null_labels(train_data_label3, 3)\n",
    "train_data_label3, valid_data_label3, test_data = fill_null_features(train_data_label3, valid_data_label3, test_data)\n",
    "train_features_label3, train_label3, valid_features_label3, valid_label3, test_features_label3 = split_features_labels(train_data_label3, valid_data_label3, test_data, 3)\n",
    "plot_label(train_label3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_train_features_label3, standardized_valid_features_label3, standardized_test_features_label3 = standardize_data(train_features_label3, valid_features_label3, test_features_label3, StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train_features_label3, pca_valid_features_label3, pca_test_features_label3 = apply_PCA(standardized_train_features_label3, standardized_valid_features_label3, standardized_test_features_label3, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_grid_params, knn_grid_params, random_forest_grid_params = get_hyper_params()\n",
    "# tune_hyper_params(svm_grid_params, knn_grid_params, random_forest_grid_params, pca_train_features_label3, train_label3, rand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = train_model(standardized_train_features_label3, train_label3, standardized_valid_features_label3, valid_label3, SVC(kernel='rbf', C=1000, gamma=0.001, class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_label3 = get_test_result(standardized_train_features_label3, train_label3, standardized_test_features_label3, model3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = 'D:\\\\ACADEMIC\\\\SEMESTER 07\\\\ML - 3\\\\project\\\\Layer07\\\\results\\\\03.csv'\n",
    "\n",
    "create_csv(test_IDs, y_pred_test_label3, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_label4 = clean_null_labels(train_data_label4, 4)\n",
    "train_data_label4, valid_data_label4, test_data = fill_null_features(train_data_label4, valid_data_label4, test_data)\n",
    "train_features_label4, train_label4, valid_features_label4, valid_label4, test_features_label4 = split_features_labels(train_data_label4, valid_data_label4, test_data, 4)\n",
    "plot_label(train_label4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_train_features_label4, standardized_valid_features_label4, standardized_test_features_label4 = standardize_data(train_features_label4, valid_features_label4, test_features_label4, StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train_features_label4, pca_valid_features_label4, pca_test_features_label4 = apply_PCA(standardized_train_features_label4, standardized_valid_features_label4, standardized_test_features_label4, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_params, knn_grid_params, random_forest_grid_params = get_hyper_params()\n",
    "tune_hyper_params(svm_grid_params, knn_grid_params, random_forest_grid_params, pca_train_features_label4, train_label4, rand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = train_model(standardized_train_features_label4, train_label4, standardized_valid_features_label4, valid_label4, SVC(kernel='rbf', C=100, gamma=0.001, class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_label4 = get_test_result(standardized_train_features_label4, train_label4, standardized_test_features_label4, model4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = 'D:\\\\ACADEMIC\\\\SEMESTER 07\\\\ML - 3\\\\project\\\\Layer07\\\\results\\\\04.csv'\n",
    "\n",
    "create_csv(test_IDs, y_pred_test_label4, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSV for all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = 'D:\\\\ACADEMIC\\\\SEMESTER 07\\\\ML - 3\\\\project\\\\Layer07\\\\results\\\\190507U.csv'\n",
    "\n",
    "# create the csv output file\n",
    "create_csv(test_IDs, y_pred_test_label1, y_pred_test_label2, y_pred_test_label3, y_pred_test_label4, destination)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
